[Unit]
Description=Ollama Service - Local AI Model Inference
Documentation=https://github.com/ollama/ollama/tree/main/docs
After=network-online.target

[Service]
Type=simple
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
StandardOutput=journal
StandardError=journal

# Environment Variables
Environment="OLLAMA_HOST=127.0.0.1:11434"
Environment="OLLAMA_MODELS=/usr/share/ollama/.ollama/models"
Environment="OLLAMA_KEEP_ALIVE=5m"
Environment="OLLAMA_MAX_LOADED_MODELS=2"

# GPU Configuration (NVIDIA CUDA)
# Uncomment if using NVIDIA GPUs
# Environment="CUDA_VISIBLE_DEVICES=0"
# Environment="LD_LIBRARY_PATH=/usr/local/cuda/lib64"

# Security Hardening
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/usr/share/ollama
ProtectKernelTunables=true
ProtectKernelModules=true
ProtectControlGroups=true
RestrictRealtime=true
RestrictNamespaces=true
LockPersonality=true
MemoryDenyWriteExecute=false
RestrictAddressFamilies=AF_UNIX AF_INET AF_INET6
SystemCallFilter=@system-service
SystemCallErrorNumber=EPERM

# Resource Limits
LimitNOFILE=65536
LimitNPROC=4096

# Timeouts
TimeoutStartSec=120
TimeoutStopSec=30

[Install]
WantedBy=multi-user.target
